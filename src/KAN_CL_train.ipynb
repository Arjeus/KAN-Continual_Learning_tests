{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T13:41:19.977663Z",
     "start_time": "2024-08-01T13:41:06.105256Z"
    },
    "id": "a-td3yfwzemE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import device\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from models import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join('..', 'results', 'mnist') # remove or add 'convs'\n",
    "strategy = \"classIL\"     # [\"taskIL\", \"classIL\"] classIL is harder \n",
    "lrs = [1e-2]\n",
    "decays = [0.8]\n",
    "epochss = [10]\n",
    "models = [Efficient_KAN_Fix(strategy, device)]\n",
    "longer_last_tasks = False\n",
    "reverse_taks = False\n",
    "\n",
    "reverse_path = \"\"\n",
    "if reverse_taks:\n",
    "    reverse_path = \"reverse_tasks\"\n",
    "longer_last_path = \"\"\n",
    "if longer_last_tasks:\n",
    "    longer_last_path = \"longer_last_tasks\"\n",
    "\n",
    "out_path = os.path.join(out_path, strategy, longer_last_path, reverse_path, 'trainings')\n",
    "cfgs = []\n",
    "for model in models[:1]:\n",
    "    for lr in lrs:\n",
    "        for decay in decays:\n",
    "            for epochs in epochss:\n",
    "                cfgs.append([model, epochs, lr, decay])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T16:32:10.634531Z",
     "start_time": "2024-08-01T16:32:10.625441Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = [datasets.MNIST, datasets.CIFAR10][0]\n",
    "dataset_name = dataset.__name__.lower()\n",
    "input_size = 28 * 28 if dataset == datasets.MNIST \\\n",
    "    else 3 * 32 * 32 if dataset == datasets.CIFAR10 \\\n",
    "    else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:23:13.200231Z",
     "start_time": "2024-08-01T17:23:08.097919Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                # transforms.Normalize((0.5,), (0.5,))\n",
    "                                ])\n",
    "# Train set. Here we sort the MNIST by digits and disable data shuffling\n",
    "train_dataset = dataset(root='../data', train=True, download=True, transform=transform)\n",
    "sorted_indices = sorted(range(len(train_dataset) // 1), key=lambda idx: train_dataset.targets[idx])\n",
    "train_subset = Subset(train_dataset, sorted_indices)\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "# MultiTask training sets\n",
    "train_loader_tasks = []\n",
    "indices = []\n",
    "for k in range(5):\n",
    "    indices.append(list(\n",
    "        filter(lambda idx: train_dataset.targets[idx] in range(k * 2, k * 2 + 2), range(len(train_dataset)))))\n",
    "    train_loader_tasks.append(\n",
    "        DataLoader(Subset(train_dataset, indices[-1]), batch_size=64, shuffle=True))\n",
    "\n",
    "# Test set\n",
    "test_dataset = dataset(root='../data', train=False, download=True, transform=transform)\n",
    "test_subset = Subset(test_dataset, range(len(test_dataset) // 1))\n",
    "test_loader = DataLoader(test_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "if reverse_taks:\n",
    "    train_loader_tasks.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats = [0 for i in range(10)]\n",
    "# for sample in test_dataset:\n",
    "#     stats[sample[1]] += 1\n",
    "# print(stats)\n",
    "# mean = sum(stats)/len(stats)\n",
    "# variance = sum([((x - mean) ** 2) for x in stats]) / len(stats) \n",
    "# res = variance ** 0.5\n",
    "# print(mean, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Trainset visualizer\n",
    "The following code prints the images of the 5 domain IL scenarios. This way we can clearly see that for the MNIST dataset each task contains a pair of digits (0-1, 2-3, etc.), while for CIFAR10 each task contains a pair of objects (car-airplane, bird-dog, deer-dog, frog-horse and truck-ship)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# def imshow(img):\n",
    "#     # img = (img / 2 + 0.5).numpy()\n",
    "#     img = img.numpy()\n",
    "#     plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# def show_images(class_index, num_images=16):\n",
    "#     dataiter = iter(train_loader_tasks[class_index])\n",
    "#     images, labels = next(dataiter)\n",
    "#     imshow(utils.make_grid(images))\n",
    "\n",
    "\n",
    "# for class_index in range(5):\n",
    "#     print(f\"TASK ID = {class_index}\")\n",
    "#     show_images(class_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, save_dir, optimizer, lr, on_epoch_end, start_epoch=0, epochs=5, isKAN=False):\n",
    "    criterion = nn.NLLLoss()\n",
    "    for epoch in range(start_epoch, epochs + start_epoch):\n",
    "        if not isKAN:\n",
    "            model.train()\n",
    "            model.to(device)\n",
    "        epoch_start = time.time_ns()\n",
    "        with tqdm(train_loader) as pbar:\n",
    "            for images, labels in pbar:\n",
    "                labels = labels.to(device)\n",
    "                images = images.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(images)\n",
    "                loss = criterion(output, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step(closure=lambda: loss)\n",
    "                accuracy = (output.argmax(dim=1) == labels).float().mean()\n",
    "                pbar.set_postfix(loss=loss.item(), accuracy=accuracy.item(), lr=optimizer.param_groups[0]['lr'])\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "        epoch_duration = (time.time_ns() - epoch_start) // 1000000\n",
    "        if on_epoch_end is not None:\n",
    "            on_epoch_end(model, save_dir, epoch, loss.item(), epoch_duration, lr, isKAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, isKAN=False):\n",
    "    if not isKAN:\n",
    "        model.eval()\n",
    "    criterion = nn.NLLLoss()\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    val_accuracy = 0\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            labels = labels.to(device)  #(labels % 2 if model.layers[-1] == 2 else labels).to(device)\n",
    "            images = images.to(device)\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            predictions.extend(output.argmax(dim=1).to('cpu').numpy())\n",
    "            ground_truths.extend(labels.to('cpu').numpy())\n",
    "            val_accuracy += (output.argmax(dim=1) == labels).float().mean().item()\n",
    "    val_accuracy /= len(test_loader)\n",
    "    print(f\"Accuracy: {val_accuracy}\")\n",
    "    return loss.item(), ground_truths, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T16:32:07.965611Z",
     "start_time": "2024-08-01T16:32:07.941654Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EpochStat:\n",
    "    @staticmethod\n",
    "    def loadModelStats(dir, name, subdir) -> list['EpochStat']:\n",
    "        return sorted([pickle.load(open(os.path.join(dir, subdir, file), 'rb')) for file in\n",
    "                       filter(lambda e: name == '_'.join(e.split('_')[:-1]), os.listdir(os.path.join(dir, subdir)))],\n",
    "                      key=lambda e: e.epoch)\n",
    "\n",
    "    def __init__(self, name, save_dir, epoch, train_loss=0, test_loss=0, labels=None, predictions=None, epoch_duration=0, lr=0):\n",
    "        self.name = name\n",
    "        self.save_dir = save_dir\n",
    "        self.train_loss = train_loss\n",
    "        self.test_loss = test_loss\n",
    "        self.epoch = epoch\n",
    "        self.predictions = predictions\n",
    "        self.labels = labels\n",
    "        self.epoch_duration = epoch_duration\n",
    "        self.lr = lr\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "\n",
    "    def save(self):\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        pickle.dump(self, open(os.path.join(self.save_dir, self.name + '_e' + str(self.epoch) + '.pickle'), 'wb'))\n",
    "\n",
    "    def get_accuracy(self):\n",
    "        accuracy = 0\n",
    "        for label, prediction in zip(self.labels, self.predictions):\n",
    "            if label == prediction:\n",
    "                accuracy += 1\n",
    "        return accuracy / len(self.labels)\n",
    "\n",
    "\n",
    "def onEpochEnd(model, save_dir, epoch, train_loss, epoch_duration, lr, isKAN):\n",
    "    test_loss, labels, predictions = test(model, isKAN)\n",
    "    stat = EpochStat(model.__class__.__name__, save_dir, epoch, train_loss, test_loss, labels, predictions, epoch_duration, lr)\n",
    "    stat.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Domain IL - training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cfg in cfgs:\n",
    "    model = cfg[0]\n",
    "    epochs = cfg[1]\n",
    "    lr = cfg[2]\n",
    "    decay_f = cfg[3]\n",
    "    if decay_f == 1:\n",
    "        lr_decay = False\n",
    "    else:\n",
    "        lr_decay = True\n",
    "    start_epochs_list = [int(epochs + epochs*i[0]) for i in enumerate(train_loader_tasks)]\n",
    "    start_epochs_list.insert(0, 0)\n",
    "    naam = model.__class__.__name__\n",
    "    isKAN = False\n",
    "    print(\"\\n\\n\", naam)\n",
    "    print(epochs, lr, decay_f, \"\\n\")\n",
    "    if 'Py_KAN' in naam:\n",
    "        isKAN = True\n",
    "    for i, task in enumerate(train_loader_tasks):\n",
    "        epochs_act = epochs\n",
    "        if longer_last_tasks and i > 3:\n",
    "            epochs_act = epochs + epochs\n",
    "\n",
    "        str_print = f'\\t\\t\\t\\tTRAINING ON TASK {i}'\n",
    "        str_print +=  f' for {epochs_act} epochs' \n",
    "        print(str_print)\n",
    "        # str_epoch = f\"ep{epochs}_10fin_\"\n",
    "        str_epoch = f\"ep{epochs}\"\n",
    "        str_lr = f\"_lr{round(math.log10(lr))}\"\n",
    "        str_decay = '_dec'+ str(decay_f) if lr_decay else ''\n",
    "        lr_act = lr * decay_f**(i)\n",
    "        train(model, os.path.join(out_path,f\"{str_epoch}{str_lr}{str_decay}\", naam), optimizer=optim.Adam(model.parameters()),\n",
    "                lr=lr_act, on_epoch_end=onEpochEnd, start_epoch=start_epochs_list[i], epochs=epochs_act, isKAN=isKAN)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T21:09:15.403425Z",
     "start_time": "2024-07-06T19:29:50.901871Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.15684713375796178\n",
      "Accuracy: 0.15684713375796178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PyKAN custom training\n",
    "for lr in [1e-0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]:\n",
    "    kan = Py_KAN()\n",
    "    test(kan)\n",
    "    # kan.train(lr=lr, train_loader=train_loader_tasks[0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
